[11/04/25 14:04:50] INFO     ğŸ“„ Created log folder:         logging_utils.py:462
                             runs/train/v9-seg-test                             
[11/04/25 14:04:50] INFO     âš¡ Using 16bit Automatic Mixed      rank_zero.py:60
                             Precision (AMP)                                    
INFO: Using 16bit Automatic Mixed Precision (AMP)
[2025-11-04 14:04:50,702][lightning.pytorch.utilities.rank_zero][INFO] - Using 16bit Automatic Mixed Precision (AMP)
                    INFO     âš¡ ğŸ’¡ Tip: For seamless cloud       rank_zero.py:60
                             uploads and versioning, try                        
                             installing                                         
                             (https://pypi.org/project/litmodels                
                             /) to enable LitModelCheckpoint,                   
                             which syncs automatically with the                 
                             Lightning model registry.                          
INFO: ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
[2025-11-04 14:04:50,705][lightning.pytorch.utilities.rank_zero][INFO] - ğŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.
                    INFO     âš¡ Trainer already configured with  rank_zero.py:60
                             model summary callbacks: [<class                   
                             'yolo.utils.logging_utils.YOLORichM                
                             odelSummary'>]. Skipping setting a                 
                             default `ModelSummary` callback.                   
INFO: Trainer already configured with model summary callbacks: [<class 'yolo.utils.logging_utils.YOLORichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
[2025-11-04 14:04:50,720][lightning.pytorch.utilities.rank_zero][INFO] - Trainer already configured with model summary callbacks: [<class 'yolo.utils.logging_utils.YOLORichModelSummary'>]. Skipping setting a default `ModelSummary` callback.
                    INFO     âš¡ GPU available: True (cuda),      rank_zero.py:60
                             used: True                                         
INFO: GPU available: True (cuda), used: True
[2025-11-04 14:04:50,759][lightning.pytorch.utilities.rank_zero][INFO] - GPU available: True (cuda), used: True
                    INFO     âš¡ TPU available: False, using: 0   rank_zero.py:60
                             TPU cores                                          
INFO: TPU available: False, using: 0 TPU cores
[2025-11-04 14:04:50,761][lightning.pytorch.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
                    INFO     âš¡ HPU available: False, using: 0   rank_zero.py:60
                             HPUs                                               
INFO: HPU available: False, using: 0 HPUs
[2025-11-04 14:04:50,762][lightning.pytorch.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
                    INFO     ğŸšœ Building YOLO                         yolo.py:35
                    INFO       ğŸ—  Building backbone                   yolo.py:38
                    INFO       ğŸ—  Building neck                       yolo.py:38
                    INFO       ğŸ—  Building head                       yolo.py:38
                    INFO       ğŸ—  Building detection                  yolo.py:38
[11/04/25 14:04:51] INFO       ğŸ—  Building auxiliary                  yolo.py:38
                    INFO     âœ… Success load model                   yolo.py:191
                    INFO     ğŸ­ Generating valid cache         data_loader.py:69
Filtering data â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:00
[11/04/25 14:04:52] INFO     Recorded 262/262 valid inputs    data_loader.py:149
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
                    INFO     ğŸ­ Generating train cache         data_loader.py:69
[11/04/25 14:04:55] WARNING  No valid BBox in                 data_loader.py:178
                             non_perio_101-tif_0_png.rf.0c932                   
                             3e8da753b4c49dcfe7e43c623bd                        
Filtering data â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 100% 0:00:01
[11/04/25 14:04:57] INFO     Recorded 1222/1223 valid inputs  data_loader.py:149
[11/04/25 14:04:58] INFO     âš¡ Initializing distributed:     distributed.py:297
                             GLOBAL_RANK: 0, MEMBER: 1/2                        
INFO: Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
[2025-11-04 14:04:58,724][lightning.fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
[11/04/25 14:05:07] INFO     âš¡ Initializing distributed:     distributed.py:297
                             GLOBAL_RANK: 1, MEMBER: 2/2                        
INFO: Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
[2025-11-04 14:05:07,011][lightning.fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
[11/04/25 14:05:07] INFO     âš¡                                  rank_zero.py:60
                             -----------------------------------                
                             -----------------------------------                
                             ------------------------------                     
                             distributed_backend=nccl                           
                             All distributed processes                          
                             registered. Starting with 2                        
                             processes                                          
                             -----------------------------------                
                             -----------------------------------                
                             ------------------------------                     
                                                                                
INFO: ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

[2025-11-04 14:05:07,334][lightning.pytorch.utilities.rank_zero][INFO] - ----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

[11/04/25 14:05:07] INFO     ğŸˆ¶ Found stride of model  bounding_box_utils.py:491
                             [8, 16, 32]                                        
                    INFO     ğŸ¨ Loading Segmentation Loss  loss_functions.py:283
                    INFO     âœ… Success load loss function loss_functions.py:336
                    INFO     âš¡ LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: cuda.py:61
                             [0,1]                                              
INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[2025-11-04 14:05:07,769][lightning.pytorch.accelerators.cuda][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
[11/04/25 14:05:07] INFO     âš¡ LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: cuda.py:61
                             [0,1]                                              
INFO: LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
[2025-11-04 14:05:07,769][lightning.pytorch.accelerators.cuda][INFO] - LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/usr/local/lib/python3.11/dist-packages/lightning/pytorch/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.
â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”“
â”ƒ   â”ƒ Name   â”ƒ Type                 â”ƒ Params â”ƒ Mode  â”ƒ
â”¡â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”©
â”‚ 0 â”‚ model  â”‚ YOLO                 â”‚ 57.9 M â”‚ train â”‚
â”‚ 1 â”‚ metric â”‚ MeanAveragePrecision â”‚      0 â”‚ train â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ Attributes                             â”ƒ Value      â”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚ Trainable params                       â”‚ 57.9 M     â”‚
â”‚ Non-trainable params                   â”‚ 96         â”‚
â”‚ Total params                           â”‚ 57.9 M     â”‚
â”‚ Total estimated model params size (MB) â”‚ 231        â”‚
â”‚ Modules in train mode                  â”‚ 1526       â”‚
â”‚ Modules in eval mode                   â”‚ 0          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Training: |          | 0/? [00:00<?, ?it/s]TRAIN START: Optimizer=AdamW, lr=0.007, weight_decay=0, amsgrad=False
[11/04/25 14:05:09] INFO     TRAIN START: Optimizer=AdamW,         solver.py:258
                             lr=0.007, weight_decay=0,                          
                             amsgrad=False                                      
TRAIN START: Optimizer=AdamW, lr=0.007, weight_decay=0, amsgrad=False
Training: |          | 0/? [00:00<?, ?it/s]Epoch 0:   0%|          | 0/153 [00:00<?, ?it/s]FIRST_BATCH DIAG: Optimizer=AdamW, lr=0.007, weight_decay=0
[11/04/25 14:05:18] INFO     FIRST_BATCH DIAG: Optimizer=AdamW,    solver.py:312
                             lr=0.007, weight_decay=0                           
FIRST_BATCH DIAG: Optimizer=AdamW, lr=0.007, weight_decay=0
[11/04/25 14:05:26] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   1%|          | 1/153 [00:17<43:12,  0.06it/s]Epoch 0:   1%|          | 1/153 [00:17<43:12,  0.06it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.740, Loss/MaskLoss_step=40.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
[11/04/25 14:05:27] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   1%|â–         | 2/153 [00:18<22:45,  0.11it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.740, Loss/MaskLoss_step=40.50]Epoch 0:   1%|â–         | 2/153 [00:18<22:45,  0.11it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.640, Loss/MaskLoss_step=40.10]AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
[11/04/25 14:05:28] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   2%|â–         | 3/153 [00:18<15:47,  0.16it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.640, Loss/MaskLoss_step=40.10]Epoch 0:   2%|â–         | 3/153 [00:18<15:47,  0.16it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=5.320, Loss/BCELoss_step=6.650, Loss/MaskLoss_step=61.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=37032, mean_grad=nan
[11/04/25 14:05:29] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=37032, mean_grad=nan                      
AFTER_BACKWARD: params_with_grad=936/942, max_grad=37032, mean_grad=nan
Epoch 0:   3%|â–         | 4/153 [00:19<12:17,  0.20it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=5.320, Loss/BCELoss_step=6.650, Loss/MaskLoss_step=61.40]Epoch 0:   3%|â–         | 4/153 [00:19<12:17,  0.20it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.730, Loss/MaskLoss_step=41.30]AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
[11/04/25 14:05:30] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   3%|â–         | 5/153 [00:20<10:16,  0.24it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.730, Loss/MaskLoss_step=41.30]Epoch 0:   3%|â–         | 5/153 [00:20<10:16,  0.24it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.610, Loss/MaskLoss_step=41.80][11/04/25 14:05:31] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=inf                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=inf
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=inf
Epoch 0:   4%|â–         | 6/153 [00:21<08:51,  0.28it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.610, Loss/MaskLoss_step=41.80]Epoch 0:   4%|â–         | 6/153 [00:21<08:51,  0.28it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.720, Loss/MaskLoss_step=40.30]AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
[11/04/25 14:05:32] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   5%|â–         | 7/153 [00:22<07:50,  0.31it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.230, Loss/BCELoss_step=6.720, Loss/MaskLoss_step=40.30]Epoch 0:   5%|â–         | 7/153 [00:22<07:50,  0.31it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.320, Loss/BCELoss_step=6.660, Loss/MaskLoss_step=66.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=27974.5, mean_grad=nan
                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=27974.5, mean_grad=nan                    
AFTER_BACKWARD: params_with_grad=936/942, max_grad=27974.5, mean_grad=nan
Epoch 0:   5%|â–Œ         | 8/153 [00:23<07:04,  0.34it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.320, Loss/BCELoss_step=6.660, Loss/MaskLoss_step=66.40]Epoch 0:   5%|â–Œ         | 8/153 [00:23<07:04,  0.34it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.600, Loss/MaskLoss_step=42.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=1830.19, mean_grad=4.95827
[11/04/25 14:05:33] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1830.19, mean_grad=4.95827                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1830.19, mean_grad=4.95827
Epoch 0:   6%|â–Œ         | 9/153 [00:24<06:36,  0.36it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.600, Loss/MaskLoss_step=42.50]Epoch 0:   6%|â–Œ         | 9/153 [00:24<06:36,  0.36it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.620, Loss/MaskLoss_step=39.30]AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
[11/04/25 14:05:35] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=inf, mean_grad=nan                        
AFTER_BACKWARD: params_with_grad=936/942, max_grad=inf, mean_grad=nan
Epoch 0:   7%|â–‹         | 10/153 [00:25<06:07,  0.39it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=5.240, Loss/BCELoss_step=6.620, Loss/MaskLoss_step=39.30]Epoch 0:   7%|â–‹         | 10/153 [00:25<06:07,  0.39it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.890, Loss/BCELoss_step=6.480, Loss/MaskLoss_step=39.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=741.141, mean_grad=1.35436
[11/04/25 14:05:36] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=741.141, mean_grad=1.35436                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=741.141, mean_grad=1.35436
Epoch 0:   7%|â–‹         | 11/153 [00:26<05:42,  0.41it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.890, Loss/BCELoss_step=6.480, Loss/MaskLoss_step=39.40]Epoch 0:   7%|â–‹         | 11/153 [00:26<05:42,  0.41it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.900, Loss/BCELoss_step=6.490, Loss/MaskLoss_step=35.30]AFTER_BACKWARD: params_with_grad=936/942, max_grad=1334, mean_grad=3.91431
[11/04/25 14:05:37] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1334, mean_grad=3.91431                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1334, mean_grad=3.91431
Epoch 0:   8%|â–Š         | 12/153 [00:27<05:22,  0.44it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.900, Loss/BCELoss_step=6.490, Loss/MaskLoss_step=35.30]Epoch 0:   8%|â–Š         | 12/153 [00:27<05:22,  0.44it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.590, Loss/BCELoss_step=6.320, Loss/MaskLoss_step=41.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=99.2031, mean_grad=0.479033
                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=99.2031, mean_grad=0.479033               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=99.2031, mean_grad=0.479033
Epoch 0:   8%|â–Š         | 13/153 [00:28<05:05,  0.46it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.590, Loss/BCELoss_step=6.320, Loss/MaskLoss_step=41.50]Epoch 0:   8%|â–Š         | 13/153 [00:28<05:05,  0.46it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.650, Loss/BCELoss_step=6.380, Loss/MaskLoss_step=35.20]AFTER_BACKWARD: params_with_grad=936/942, max_grad=103.906, mean_grad=0.345436
[11/04/25 14:05:38] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=103.906, mean_grad=0.345436               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=103.906, mean_grad=0.345436
Epoch 0:   9%|â–‰         | 14/153 [00:29<04:51,  0.48it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.650, Loss/BCELoss_step=6.380, Loss/MaskLoss_step=35.20]Epoch 0:   9%|â–‰         | 14/153 [00:29<04:51,  0.48it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.370, Loss/BCELoss_step=6.310, Loss/MaskLoss_step=36.10]AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.625, mean_grad=0.449764
[11/04/25 14:05:39] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=79.625, mean_grad=0.449764                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.625, mean_grad=0.449764
Epoch 0:  10%|â–‰         | 15/153 [00:30<04:37,  0.50it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.370, Loss/BCELoss_step=6.310, Loss/MaskLoss_step=36.10]Epoch 0:  10%|â–‰         | 15/153 [00:30<04:38,  0.50it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.340, Loss/BCELoss_step=6.170, Loss/MaskLoss_step=34.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=93.0283, mean_grad=0.37299
[11/04/25 14:05:40] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=93.0283, mean_grad=0.37299                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=93.0283, mean_grad=0.37299
Epoch 0:  10%|â–ˆ         | 16/153 [00:31<04:26,  0.51it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=4.340, Loss/BCELoss_step=6.170, Loss/MaskLoss_step=34.00]Epoch 0:  10%|â–ˆ         | 16/153 [00:31<04:26,  0.51it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.990, Loss/BCELoss_step=5.880, Loss/MaskLoss_step=35.70][11/04/25 14:05:41] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=998.25, mean_grad=1.60775                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=998.25, mean_grad=1.60775
AFTER_BACKWARD: params_with_grad=936/942, max_grad=998.25, mean_grad=1.60775
Epoch 0:  11%|â–ˆ         | 17/153 [00:31<04:15,  0.53it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.990, Loss/BCELoss_step=5.880, Loss/MaskLoss_step=35.70]Epoch 0:  11%|â–ˆ         | 17/153 [00:31<04:15,  0.53it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.520, Loss/BCELoss_step=5.810, Loss/MaskLoss_step=39.20]AFTER_BACKWARD: params_with_grad=936/942, max_grad=68.375, mean_grad=0.320019
[11/04/25 14:05:42] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=68.375, mean_grad=0.320019                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=68.375, mean_grad=0.320019
Epoch 0:  12%|â–ˆâ–        | 18/153 [00:32<04:06,  0.55it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.520, Loss/BCELoss_step=5.810, Loss/MaskLoss_step=39.20]Epoch 0:  12%|â–ˆâ–        | 18/153 [00:32<04:06,  0.55it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=3.410, Loss/BCELoss_step=5.520, Loss/MaskLoss_step=38.00][11/04/25 14:05:43] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=213.25, mean_grad=0.423238                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=213.25, mean_grad=0.423238
AFTER_BACKWARD: params_with_grad=936/942, max_grad=213.25, mean_grad=0.423238
Epoch 0:  12%|â–ˆâ–        | 19/153 [00:33<03:58,  0.56it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=3.410, Loss/BCELoss_step=5.520, Loss/MaskLoss_step=38.00]Epoch 0:  12%|â–ˆâ–        | 19/153 [00:33<03:58,  0.56it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.180, Loss/BCELoss_step=5.390, Loss/MaskLoss_step=36.50][11/04/25 14:05:44] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=79.5938, mean_grad=0.360221               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.5938, mean_grad=0.360221
AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.5938, mean_grad=0.360221
Epoch 0:  13%|â–ˆâ–        | 20/153 [00:34<03:50,  0.58it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=3.180, Loss/BCELoss_step=5.390, Loss/MaskLoss_step=36.50]Epoch 0:  13%|â–ˆâ–        | 20/153 [00:34<03:50,  0.58it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=2.990, Loss/BCELoss_step=5.270, Loss/MaskLoss_step=38.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=267, mean_grad=0.518277
[11/04/25 14:05:45] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=267, mean_grad=0.518277                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=267, mean_grad=0.518277
Epoch 0:  14%|â–ˆâ–        | 21/153 [00:35<03:43,  0.59it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=2.990, Loss/BCELoss_step=5.270, Loss/MaskLoss_step=38.90]Epoch 0:  14%|â–ˆâ–        | 21/153 [00:35<03:43,  0.59it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=2.360, Loss/BCELoss_step=5.280, Loss/MaskLoss_step=33.10]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=94.0625, mean_grad=0.263815               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=94.0625, mean_grad=0.263815
AFTER_BACKWARD: params_with_grad=936/942, max_grad=94.0625, mean_grad=0.263815
Epoch 0:  14%|â–ˆâ–        | 22/153 [00:36<03:36,  0.60it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=2.360, Loss/BCELoss_step=5.280, Loss/MaskLoss_step=33.10]Epoch 0:  14%|â–ˆâ–        | 22/153 [00:36<03:36,  0.60it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.850, Loss/BCELoss_step=5.210, Loss/MaskLoss_step=34.40][11/04/25 14:05:46] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=251.812, mean_grad=0.462248               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=251.812, mean_grad=0.462248
AFTER_BACKWARD: params_with_grad=936/942, max_grad=251.812, mean_grad=0.462248
Epoch 0:  15%|â–ˆâ–Œ        | 23/153 [00:37<03:30,  0.62it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.850, Loss/BCELoss_step=5.210, Loss/MaskLoss_step=34.40]Epoch 0:  15%|â–ˆâ–Œ        | 23/153 [00:37<03:30,  0.62it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.860, Loss/BCELoss_step=4.940, Loss/MaskLoss_step=36.10]AFTER_BACKWARD: params_with_grad=936/942, max_grad=179.375, mean_grad=0.369901
[11/04/25 14:05:47] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=179.375, mean_grad=0.369901               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=179.375, mean_grad=0.369901
Epoch 0:  16%|â–ˆâ–Œ        | 24/153 [00:38<03:25,  0.63it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.860, Loss/BCELoss_step=4.940, Loss/MaskLoss_step=36.10]Epoch 0:  16%|â–ˆâ–Œ        | 24/153 [00:38<03:25,  0.63it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.880, Loss/BCELoss_step=5.010, Loss/MaskLoss_step=32.60]AFTER_BACKWARD: params_with_grad=936/942, max_grad=105.719, mean_grad=0.271354
[11/04/25 14:05:48] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=105.719, mean_grad=0.271354               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=105.719, mean_grad=0.271354
Epoch 0:  16%|â–ˆâ–‹        | 25/153 [00:39<03:19,  0.64it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.880, Loss/BCELoss_step=5.010, Loss/MaskLoss_step=32.60]Epoch 0:  16%|â–ˆâ–‹        | 25/153 [00:39<03:19,  0.64it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.610, Loss/BCELoss_step=4.870, Loss/MaskLoss_step=30.80]AFTER_BACKWARD: params_with_grad=936/942, max_grad=142.125, mean_grad=0.287186
[11/04/25 14:05:49] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=142.125, mean_grad=0.287186               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=142.125, mean_grad=0.287186
Epoch 0:  17%|â–ˆâ–‹        | 26/153 [00:39<03:14,  0.65it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.610, Loss/BCELoss_step=4.870, Loss/MaskLoss_step=30.80]Epoch 0:  17%|â–ˆâ–‹        | 26/153 [00:39<03:14,  0.65it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.500, Loss/BCELoss_step=4.640, Loss/MaskLoss_step=31.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=286.375, mean_grad=0.471743
[11/04/25 14:05:50] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=286.375, mean_grad=0.471743               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=286.375, mean_grad=0.471743
Epoch 0:  18%|â–ˆâ–Š        | 27/153 [00:40<03:10,  0.66it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.500, Loss/BCELoss_step=4.640, Loss/MaskLoss_step=31.50]Epoch 0:  18%|â–ˆâ–Š        | 27/153 [00:40<03:10,  0.66it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.400, Loss/BCELoss_step=4.670, Loss/MaskLoss_step=30.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=240.688, mean_grad=0.452883
[11/04/25 14:05:51] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=240.688, mean_grad=0.452883               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=240.688, mean_grad=0.452883
Epoch 0:  18%|â–ˆâ–Š        | 28/153 [00:41<03:05,  0.67it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.400, Loss/BCELoss_step=4.670, Loss/MaskLoss_step=30.00]Epoch 0:  18%|â–ˆâ–Š        | 28/153 [00:41<03:05,  0.67it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.470, Loss/BCELoss_step=4.700, Loss/MaskLoss_step=35.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=214.188, mean_grad=0.372094
[11/04/25 14:05:52] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=214.188, mean_grad=0.372094               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=214.188, mean_grad=0.372094
Epoch 0:  19%|â–ˆâ–‰        | 29/153 [00:42<03:01,  0.68it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.470, Loss/BCELoss_step=4.700, Loss/MaskLoss_step=35.90]Epoch 0:  19%|â–ˆâ–‰        | 29/153 [00:42<03:01,  0.68it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.170, Loss/BCELoss_step=4.580, Loss/MaskLoss_step=30.70]AFTER_BACKWARD: params_with_grad=936/942, max_grad=181.172, mean_grad=0.406059
                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=181.172, mean_grad=0.406059               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=181.172, mean_grad=0.406059
Epoch 0:  20%|â–ˆâ–‰        | 30/153 [00:43<02:57,  0.69it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.170, Loss/BCELoss_step=4.580, Loss/MaskLoss_step=30.70]Epoch 0:  20%|â–ˆâ–‰        | 30/153 [00:43<02:57,  0.69it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=1.190, Loss/BCELoss_step=4.520, Loss/MaskLoss_step=33.80]AFTER_BACKWARD: params_with_grad=936/942, max_grad=186.312, mean_grad=0.465194
[11/04/25 14:05:53] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=186.312, mean_grad=0.465194               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=186.312, mean_grad=0.465194
Epoch 0:  20%|â–ˆâ–ˆ        | 31/153 [00:44<02:54,  0.70it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=1.190, Loss/BCELoss_step=4.520, Loss/MaskLoss_step=33.80]Epoch 0:  20%|â–ˆâ–ˆ        | 31/153 [00:44<02:54,  0.70it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.400, Loss/BCELoss_step=4.610, Loss/MaskLoss_step=30.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=310.438, mean_grad=0.410088
[11/04/25 14:05:54] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=310.438, mean_grad=0.410088               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=310.438, mean_grad=0.410088
Epoch 0:  21%|â–ˆâ–ˆ        | 32/153 [00:45<02:50,  0.71it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.400, Loss/BCELoss_step=4.610, Loss/MaskLoss_step=30.50]Epoch 0:  21%|â–ˆâ–ˆ        | 32/153 [00:45<02:50,  0.71it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.210, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=33.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.625, mean_grad=0.672336
[11/04/25 14:05:55] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=278.625, mean_grad=0.672336               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.625, mean_grad=0.672336
Epoch 0:  22%|â–ˆâ–ˆâ–       | 33/153 [00:45<02:47,  0.72it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.210, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=33.00]Epoch 0:  22%|â–ˆâ–ˆâ–       | 33/153 [00:45<02:47,  0.72it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.030, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=33.60]AFTER_BACKWARD: params_with_grad=936/942, max_grad=100.59, mean_grad=0.269429
[11/04/25 14:05:56] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=100.59, mean_grad=0.269429                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=100.59, mean_grad=0.269429
Epoch 0:  22%|â–ˆâ–ˆâ–       | 34/153 [00:46<02:43,  0.73it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.030, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=33.60]Epoch 0:  22%|â–ˆâ–ˆâ–       | 34/153 [00:46<02:44,  0.73it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.030, Loss/BCELoss_step=4.380, Loss/MaskLoss_step=28.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=96, mean_grad=0.25184
[11/04/25 14:05:57] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=96, mean_grad=0.25184                     
AFTER_BACKWARD: params_with_grad=936/942, max_grad=96, mean_grad=0.25184
Epoch 0:  23%|â–ˆâ–ˆâ–       | 35/153 [00:47<02:40,  0.73it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=1.030, Loss/BCELoss_step=4.380, Loss/MaskLoss_step=28.50]Epoch 0:  23%|â–ˆâ–ˆâ–       | 35/153 [00:47<02:40,  0.73it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.967, Loss/BCELoss_step=4.340, Loss/MaskLoss_step=29.30]AFTER_BACKWARD: params_with_grad=936/942, max_grad=281.188, mean_grad=0.39047
[11/04/25 14:05:58] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=281.188, mean_grad=0.39047                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=281.188, mean_grad=0.39047
Epoch 0:  24%|â–ˆâ–ˆâ–       | 36/153 [00:48<02:37,  0.74it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.967, Loss/BCELoss_step=4.340, Loss/MaskLoss_step=29.30]Epoch 0:  24%|â–ˆâ–ˆâ–       | 36/153 [00:48<02:37,  0.74it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.887, Loss/BCELoss_step=4.320, Loss/MaskLoss_step=33.60]AFTER_BACKWARD: params_with_grad=936/942, max_grad=151.125, mean_grad=0.332237
[11/04/25 14:05:59] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=151.125, mean_grad=0.332237               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=151.125, mean_grad=0.332237
Epoch 0:  24%|â–ˆâ–ˆâ–       | 37/153 [00:49<02:35,  0.75it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.887, Loss/BCELoss_step=4.320, Loss/MaskLoss_step=33.60]Epoch 0:  24%|â–ˆâ–ˆâ–       | 37/153 [00:49<02:35,  0.75it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.848, Loss/BCELoss_step=4.110, Loss/MaskLoss_step=33.60]AFTER_BACKWARD: params_with_grad=936/942, max_grad=136.281, mean_grad=0.413087
                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=136.281, mean_grad=0.413087               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=136.281, mean_grad=0.413087
Epoch 0:  25%|â–ˆâ–ˆâ–       | 38/153 [00:50<02:32,  0.75it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.848, Loss/BCELoss_step=4.110, Loss/MaskLoss_step=33.60]Epoch 0:  25%|â–ˆâ–ˆâ–       | 38/153 [00:50<02:32,  0.75it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.818, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=25.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=106.891, mean_grad=0.35391
[11/04/25 14:06:00] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=106.891, mean_grad=0.35391                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=106.891, mean_grad=0.35391
Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 39/153 [00:51<02:29,  0.76it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.818, Loss/BCELoss_step=4.370, Loss/MaskLoss_step=25.90]Epoch 0:  25%|â–ˆâ–ˆâ–Œ       | 39/153 [00:51<02:29,  0.76it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.874, Loss/BCELoss_step=4.220, Loss/MaskLoss_step=30.00][11/04/25 14:06:01] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=188.812, mean_grad=0.476444               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=188.812, mean_grad=0.476444
AFTER_BACKWARD: params_with_grad=936/942, max_grad=188.812, mean_grad=0.476444
Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 40/153 [00:52<02:27,  0.77it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.874, Loss/BCELoss_step=4.220, Loss/MaskLoss_step=30.00]Epoch 0:  26%|â–ˆâ–ˆâ–Œ       | 40/153 [00:52<02:27,  0.77it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.664, Loss/BCELoss_step=4.290, Loss/MaskLoss_step=29.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=299.125, mean_grad=0.370521
[11/04/25 14:06:02] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=299.125, mean_grad=0.370521               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=299.125, mean_grad=0.370521
Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 41/153 [00:53<02:24,  0.77it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.664, Loss/BCELoss_step=4.290, Loss/MaskLoss_step=29.90]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 41/153 [00:53<02:24,  0.77it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.765, Loss/BCELoss_step=4.330, Loss/MaskLoss_step=28.70][11/04/25 14:06:03] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=143.688, mean_grad=0.344542               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=143.688, mean_grad=0.344542
AFTER_BACKWARD: params_with_grad=936/942, max_grad=143.688, mean_grad=0.344542
Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 42/153 [00:53<02:22,  0.78it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.765, Loss/BCELoss_step=4.330, Loss/MaskLoss_step=28.70]Epoch 0:  27%|â–ˆâ–ˆâ–‹       | 42/153 [00:53<02:22,  0.78it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.605, Loss/BCELoss_step=4.240, Loss/MaskLoss_step=27.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=228.562, mean_grad=0.296832
[11/04/25 14:06:04] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=228.562, mean_grad=0.296832               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=228.562, mean_grad=0.296832
Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 43/153 [00:54<02:20,  0.79it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.605, Loss/BCELoss_step=4.240, Loss/MaskLoss_step=27.00]Epoch 0:  28%|â–ˆâ–ˆâ–Š       | 43/153 [00:54<02:20,  0.79it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.565, Loss/BCELoss_step=4.040, Loss/MaskLoss_step=26.80]AFTER_BACKWARD: params_with_grad=936/942, max_grad=194.812, mean_grad=0.31023
[11/04/25 14:06:05] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=194.812, mean_grad=0.31023                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=194.812, mean_grad=0.31023
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 44/153 [00:55<02:17,  0.79it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.565, Loss/BCELoss_step=4.040, Loss/MaskLoss_step=26.80]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 44/153 [00:55<02:17,  0.79it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.667, Loss/BCELoss_step=4.040, Loss/MaskLoss_step=30.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=167.547, mean_grad=0.304454
[11/04/25 14:06:06] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=167.547, mean_grad=0.304454               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=167.547, mean_grad=0.304454
Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 45/153 [00:56<02:15,  0.80it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.667, Loss/BCELoss_step=4.040, Loss/MaskLoss_step=30.90]Epoch 0:  29%|â–ˆâ–ˆâ–‰       | 45/153 [00:56<02:15,  0.80it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.614, Loss/BCELoss_step=4.120, Loss/MaskLoss_step=24.80]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=53.6875, mean_grad=0.281326               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=53.6875, mean_grad=0.281326
AFTER_BACKWARD: params_with_grad=936/942, max_grad=53.6875, mean_grad=0.281326
Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 46/153 [00:57<02:13,  0.80it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.614, Loss/BCELoss_step=4.120, Loss/MaskLoss_step=24.80]Epoch 0:  30%|â–ˆâ–ˆâ–ˆ       | 46/153 [00:57<02:13,  0.80it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.511, Loss/BCELoss_step=4.030, Loss/MaskLoss_step=26.70][11/04/25 14:06:07] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=263.453, mean_grad=0.38204                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=263.453, mean_grad=0.38204
AFTER_BACKWARD: params_with_grad=936/942, max_grad=263.453, mean_grad=0.38204
Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 47/153 [00:58<02:11,  0.81it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.511, Loss/BCELoss_step=4.030, Loss/MaskLoss_step=26.70]Epoch 0:  31%|â–ˆâ–ˆâ–ˆ       | 47/153 [00:58<02:11,  0.81it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.500, Loss/BCELoss_step=4.010, Loss/MaskLoss_step=28.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=300.062, mean_grad=0.432529
[11/04/25 14:06:08] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=300.062, mean_grad=0.432529               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=300.062, mean_grad=0.432529
Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 48/153 [00:59<02:09,  0.81it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.500, Loss/BCELoss_step=4.010, Loss/MaskLoss_step=28.00]Epoch 0:  31%|â–ˆâ–ˆâ–ˆâ–      | 48/153 [00:59<02:09,  0.81it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.497, Loss/BCELoss_step=3.950, Loss/MaskLoss_step=29.80]AFTER_BACKWARD: params_with_grad=936/942, max_grad=304.625, mean_grad=0.395555
[11/04/25 14:06:09] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=304.625, mean_grad=0.395555               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=304.625, mean_grad=0.395555
Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/153 [01:00<02:07,  0.82it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.497, Loss/BCELoss_step=3.950, Loss/MaskLoss_step=29.80]Epoch 0:  32%|â–ˆâ–ˆâ–ˆâ–      | 49/153 [01:00<02:07,  0.82it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.504, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=31.50][11/04/25 14:06:10] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=281.5, mean_grad=0.439533                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=281.5, mean_grad=0.439533
AFTER_BACKWARD: params_with_grad=936/942, max_grad=281.5, mean_grad=0.439533
Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/153 [01:00<02:05,  0.82it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.504, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=31.50]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 50/153 [01:00<02:05,  0.82it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.501, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=30.20][11/04/25 14:06:11] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=102.25, mean_grad=0.407863                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=102.25, mean_grad=0.407863
AFTER_BACKWARD: params_with_grad=936/942, max_grad=102.25, mean_grad=0.407863
Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 51/153 [01:01<02:03,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.501, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=30.20]Epoch 0:  33%|â–ˆâ–ˆâ–ˆâ–      | 51/153 [01:01<02:03,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.396, Loss/BCELoss_step=3.820, Loss/MaskLoss_step=26.30][11/04/25 14:06:12] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=136.406, mean_grad=0.526676               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=136.406, mean_grad=0.526676
AFTER_BACKWARD: params_with_grad=936/942, max_grad=136.406, mean_grad=0.526676
Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/153 [01:02<02:01,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.396, Loss/BCELoss_step=3.820, Loss/MaskLoss_step=26.30]Epoch 0:  34%|â–ˆâ–ˆâ–ˆâ–      | 52/153 [01:02<02:01,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.403, Loss/BCELoss_step=3.700, Loss/MaskLoss_step=27.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=284.812, mean_grad=0.440384
[11/04/25 14:06:13] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=284.812, mean_grad=0.440384               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=284.812, mean_grad=0.440384
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 53/153 [01:03<01:59,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.403, Loss/BCELoss_step=3.700, Loss/MaskLoss_step=27.40]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–      | 53/153 [01:03<01:59,  0.83it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.429, Loss/BCELoss_step=3.840, Loss/MaskLoss_step=29.60][11/04/25 14:06:14] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=81.125, mean_grad=0.380472                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=81.125, mean_grad=0.380472
AFTER_BACKWARD: params_with_grad=936/942, max_grad=81.125, mean_grad=0.380472
Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/153 [01:04<01:58,  0.84it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.429, Loss/BCELoss_step=3.840, Loss/MaskLoss_step=29.60]Epoch 0:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 54/153 [01:04<01:58,  0.84it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.412, Loss/BCELoss_step=3.700, Loss/MaskLoss_step=26.80]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=112.406, mean_grad=0.388106               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=112.406, mean_grad=0.388106
AFTER_BACKWARD: params_with_grad=936/942, max_grad=112.406, mean_grad=0.388106
Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 55/153 [01:05<01:56,  0.84it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.412, Loss/BCELoss_step=3.700, Loss/MaskLoss_step=26.80]Epoch 0:  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 55/153 [01:05<01:56,  0.84it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.386, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=24.70]AFTER_BACKWARD: params_with_grad=936/942, max_grad=133.266, mean_grad=0.396644
[11/04/25 14:06:15] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=133.266, mean_grad=0.396644               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=133.266, mean_grad=0.396644
Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/153 [01:06<01:54,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.386, Loss/BCELoss_step=3.980, Loss/MaskLoss_step=24.70]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 56/153 [01:06<01:54,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.397, Loss/BCELoss_step=3.720, Loss/MaskLoss_step=28.00][11/04/25 14:06:16] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=79.3125, mean_grad=0.320459               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.3125, mean_grad=0.320459
AFTER_BACKWARD: params_with_grad=936/942, max_grad=79.3125, mean_grad=0.320459
Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 57/153 [01:07<01:53,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.397, Loss/BCELoss_step=3.720, Loss/MaskLoss_step=28.00]Epoch 0:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 57/153 [01:07<01:53,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.320, Loss/BCELoss_step=4.080, Loss/MaskLoss_step=25.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=68.3906, mean_grad=0.328495
[11/04/25 14:06:17] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=68.3906, mean_grad=0.328495               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=68.3906, mean_grad=0.328495
Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/153 [01:08<01:51,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.320, Loss/BCELoss_step=4.080, Loss/MaskLoss_step=25.90]Epoch 0:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 58/153 [01:08<01:51,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.415, Loss/BCELoss_step=3.890, Loss/MaskLoss_step=25.60][11/04/25 14:06:18] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=168.562, mean_grad=0.495125               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=168.562, mean_grad=0.495125
AFTER_BACKWARD: params_with_grad=936/942, max_grad=168.562, mean_grad=0.495125
Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 59/153 [01:09<01:50,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.415, Loss/BCELoss_step=3.890, Loss/MaskLoss_step=25.60]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 59/153 [01:09<01:50,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.362, Loss/BCELoss_step=3.630, Loss/MaskLoss_step=28.20][11/04/25 14:06:19] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=110.75, mean_grad=0.376725                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=110.75, mean_grad=0.376725
AFTER_BACKWARD: params_with_grad=936/942, max_grad=110.75, mean_grad=0.376725
Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 60/153 [01:10<01:48,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.362, Loss/BCELoss_step=3.630, Loss/MaskLoss_step=28.20]Epoch 0:  39%|â–ˆâ–ˆâ–ˆâ–‰      | 60/153 [01:10<01:48,  0.85it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.820, Loss/MaskLoss_step=26.00][11/04/25 14:06:20] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=145.062, mean_grad=0.357115               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=145.062, mean_grad=0.357115
AFTER_BACKWARD: params_with_grad=936/942, max_grad=145.062, mean_grad=0.357115
Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 61/153 [01:11<01:47,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.820, Loss/MaskLoss_step=26.00]Epoch 0:  40%|â–ˆâ–ˆâ–ˆâ–‰      | 61/153 [01:11<01:47,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.332, Loss/BCELoss_step=3.460, Loss/MaskLoss_step=25.60][11/04/25 14:06:21] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=113.312, mean_grad=0.405425               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=113.312, mean_grad=0.405425
AFTER_BACKWARD: params_with_grad=936/942, max_grad=113.312, mean_grad=0.405425
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/153 [01:11<01:45,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.332, Loss/BCELoss_step=3.460, Loss/MaskLoss_step=25.60]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 62/153 [01:12<01:45,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.352, Loss/BCELoss_step=3.690, Loss/MaskLoss_step=25.70][11/04/25 14:06:22] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=193.438, mean_grad=0.665966               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=193.438, mean_grad=0.665966
AFTER_BACKWARD: params_with_grad=936/942, max_grad=193.438, mean_grad=0.665966
Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 63/153 [01:12<01:44,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.352, Loss/BCELoss_step=3.690, Loss/MaskLoss_step=25.70]Epoch 0:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 63/153 [01:12<01:44,  0.86it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.329, Loss/BCELoss_step=3.760, Loss/MaskLoss_step=29.40][11/04/25 14:06:23] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=155.719, mean_grad=0.544843               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=155.719, mean_grad=0.544843
AFTER_BACKWARD: params_with_grad=936/942, max_grad=155.719, mean_grad=0.544843
Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/153 [01:13<01:42,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.329, Loss/BCELoss_step=3.760, Loss/MaskLoss_step=29.40]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64/153 [01:13<01:42,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.580, Loss/MaskLoss_step=25.30][11/04/25 14:06:24] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=439.75, mean_grad=0.885203                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=439.75, mean_grad=0.885203
AFTER_BACKWARD: params_with_grad=936/942, max_grad=439.75, mean_grad=0.885203
Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/153 [01:14<01:41,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.580, Loss/MaskLoss_step=25.30]Epoch 0:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65/153 [01:14<01:41,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.326, Loss/BCELoss_step=3.570, Loss/MaskLoss_step=24.40][11/04/25 14:06:25] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=332.062, mean_grad=0.632472               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=332.062, mean_grad=0.632472
AFTER_BACKWARD: params_with_grad=936/942, max_grad=332.062, mean_grad=0.632472
Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/153 [01:15<01:39,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.326, Loss/BCELoss_step=3.570, Loss/MaskLoss_step=24.40]Epoch 0:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 66/153 [01:15<01:39,  0.87it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.276, Loss/BCELoss_step=3.640, Loss/MaskLoss_step=26.00]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=301.562, mean_grad=0.485529               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=301.562, mean_grad=0.485529
AFTER_BACKWARD: params_with_grad=936/942, max_grad=301.562, mean_grad=0.485529
Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/153 [01:16<01:38,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.276, Loss/BCELoss_step=3.640, Loss/MaskLoss_step=26.00]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 67/153 [01:16<01:38,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.680, Loss/MaskLoss_step=24.80][11/04/25 14:06:26] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=111.781, mean_grad=0.434074               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=111.781, mean_grad=0.434074
AFTER_BACKWARD: params_with_grad=936/942, max_grad=111.781, mean_grad=0.434074
Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/153 [01:17<01:36,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.680, Loss/MaskLoss_step=24.80]Epoch 0:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 68/153 [01:17<01:36,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.261, Loss/BCELoss_step=3.690, Loss/MaskLoss_step=24.40][11/04/25 14:06:27] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=541, mean_grad=0.636549                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=541, mean_grad=0.636549
AFTER_BACKWARD: params_with_grad=936/942, max_grad=541, mean_grad=0.636549
Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/153 [01:18<01:35,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.261, Loss/BCELoss_step=3.690, Loss/MaskLoss_step=24.40]Epoch 0:  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 69/153 [01:18<01:35,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.208, Loss/BCELoss_step=3.550, Loss/MaskLoss_step=30.50][11/04/25 14:06:28] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=168.062, mean_grad=0.512743               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=168.062, mean_grad=0.512743
AFTER_BACKWARD: params_with_grad=936/942, max_grad=168.062, mean_grad=0.512743
Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 70/153 [01:19<01:33,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.208, Loss/BCELoss_step=3.550, Loss/MaskLoss_step=30.50]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 70/153 [01:19<01:33,  0.88it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.620, Loss/MaskLoss_step=21.40][11/04/25 14:06:29] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=405.5, mean_grad=0.483021                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=405.5, mean_grad=0.483021
AFTER_BACKWARD: params_with_grad=936/942, max_grad=405.5, mean_grad=0.483021
Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/153 [01:19<01:32,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.620, Loss/MaskLoss_step=21.40]Epoch 0:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 71/153 [01:20<01:32,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.233, Loss/BCELoss_step=3.340, Loss/MaskLoss_step=24.10][11/04/25 14:06:30] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=349.125, mean_grad=0.495289               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=349.125, mean_grad=0.495289
AFTER_BACKWARD: params_with_grad=936/942, max_grad=349.125, mean_grad=0.495289
Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 72/153 [01:20<01:30,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.233, Loss/BCELoss_step=3.340, Loss/MaskLoss_step=24.10]Epoch 0:  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 72/153 [01:20<01:31,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.330, Loss/BCELoss_step=3.500, Loss/MaskLoss_step=24.40][11/04/25 14:06:31] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=534.75, mean_grad=0.84428                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=534.75, mean_grad=0.84428
AFTER_BACKWARD: params_with_grad=936/942, max_grad=534.75, mean_grad=0.84428
Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/153 [01:21<01:29,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.330, Loss/BCELoss_step=3.500, Loss/MaskLoss_step=24.40]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 73/153 [01:21<01:29,  0.89it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.243, Loss/BCELoss_step=3.470, Loss/MaskLoss_step=23.90][11/04/25 14:06:32] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=243.688, mean_grad=0.618099               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=243.688, mean_grad=0.618099
AFTER_BACKWARD: params_with_grad=936/942, max_grad=243.688, mean_grad=0.618099
Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 74/153 [01:22<01:28,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.243, Loss/BCELoss_step=3.470, Loss/MaskLoss_step=23.90]Epoch 0:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 74/153 [01:22<01:28,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.337, Loss/BCELoss_step=3.560, Loss/MaskLoss_step=27.20][11/04/25 14:06:33] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=617.5, mean_grad=0.686854                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=617.5, mean_grad=0.686854
AFTER_BACKWARD: params_with_grad=936/942, max_grad=617.5, mean_grad=0.686854
Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/153 [01:23<01:26,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.337, Loss/BCELoss_step=3.560, Loss/MaskLoss_step=27.20]Epoch 0:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 75/153 [01:23<01:26,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.281, Loss/BCELoss_step=3.520, Loss/MaskLoss_step=23.50][11/04/25 14:06:34] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=397, mean_grad=0.759412                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=397, mean_grad=0.759412
AFTER_BACKWARD: params_with_grad=936/942, max_grad=397, mean_grad=0.759412
Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 76/153 [01:24<01:25,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.281, Loss/BCELoss_step=3.520, Loss/MaskLoss_step=23.50]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 76/153 [01:24<01:25,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.263, Loss/BCELoss_step=3.620, Loss/MaskLoss_step=23.50]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=838, mean_grad=0.862584                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=838, mean_grad=0.862584
AFTER_BACKWARD: params_with_grad=936/942, max_grad=838, mean_grad=0.862584
Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/153 [01:25<01:24,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.263, Loss/BCELoss_step=3.620, Loss/MaskLoss_step=23.50]Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 77/153 [01:25<01:24,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.250, Loss/BCELoss_step=3.590, Loss/MaskLoss_step=20.80][11/04/25 14:06:35] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=278.594, mean_grad=0.459502               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.594, mean_grad=0.459502
AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.594, mean_grad=0.459502
Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 78/153 [01:26<01:22,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.250, Loss/BCELoss_step=3.590, Loss/MaskLoss_step=20.80]Epoch 0:  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 78/153 [01:26<01:22,  0.90it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.257, Loss/BCELoss_step=3.440, Loss/MaskLoss_step=22.50][11/04/25 14:06:36] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=365.75, mean_grad=0.550603                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=365.75, mean_grad=0.550603
AFTER_BACKWARD: params_with_grad=936/942, max_grad=365.75, mean_grad=0.550603
Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/153 [01:27<01:21,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.257, Loss/BCELoss_step=3.440, Loss/MaskLoss_step=22.50]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79/153 [01:27<01:21,  0.91it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.236, Loss/BCELoss_step=3.320, Loss/MaskLoss_step=23.20][11/04/25 14:06:37] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=366.125, mean_grad=0.555681               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=366.125, mean_grad=0.555681
AFTER_BACKWARD: params_with_grad=936/942, max_grad=366.125, mean_grad=0.555681
Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/153 [01:28<01:20,  0.91it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.236, Loss/BCELoss_step=3.320, Loss/MaskLoss_step=23.20]Epoch 0:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 80/153 [01:28<01:20,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.230, Loss/MaskLoss_step=25.90][11/04/25 14:06:38] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=607.25, mean_grad=0.684882                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=607.25, mean_grad=0.684882
AFTER_BACKWARD: params_with_grad=936/942, max_grad=607.25, mean_grad=0.684882
Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/153 [01:28<01:19,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.230, Loss/MaskLoss_step=25.90]Epoch 0:  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 81/153 [01:28<01:19,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.218, Loss/BCELoss_step=3.350, Loss/MaskLoss_step=24.40][11/04/25 14:06:39] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=214.438, mean_grad=0.476905               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=214.438, mean_grad=0.476905
AFTER_BACKWARD: params_with_grad=936/942, max_grad=214.438, mean_grad=0.476905
Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/153 [01:29<01:17,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.218, Loss/BCELoss_step=3.350, Loss/MaskLoss_step=24.40]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 82/153 [01:29<01:17,  0.91it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.490, Loss/MaskLoss_step=25.90]AFTER_BACKWARD: params_with_grad=936/942, max_grad=693.875, mean_grad=0.590876
[11/04/25 14:06:40] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=693.875, mean_grad=0.590876               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=693.875, mean_grad=0.590876
Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/153 [01:30<01:16,  0.91it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.324, Loss/BCELoss_step=3.490, Loss/MaskLoss_step=25.90]Epoch 0:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 83/153 [01:30<01:16,  0.91it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.236, Loss/BCELoss_step=3.260, Loss/MaskLoss_step=29.00][11/04/25 14:06:41] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=524.125, mean_grad=0.794241               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=524.125, mean_grad=0.794241
AFTER_BACKWARD: params_with_grad=936/942, max_grad=524.125, mean_grad=0.794241
Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 84/153 [01:31<01:15,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.236, Loss/BCELoss_step=3.260, Loss/MaskLoss_step=29.00]Epoch 0:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 84/153 [01:31<01:15,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.430, Loss/MaskLoss_step=23.60][11/04/25 14:06:42] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=307.75, mean_grad=0.487913                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=307.75, mean_grad=0.487913
AFTER_BACKWARD: params_with_grad=936/942, max_grad=307.75, mean_grad=0.487913
Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 85/153 [01:32<01:14,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.430, Loss/MaskLoss_step=23.60]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 85/153 [01:32<01:14,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.264, Loss/BCELoss_step=3.650, Loss/MaskLoss_step=22.60][11/04/25 14:06:43] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=482.625, mean_grad=0.661465               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=482.625, mean_grad=0.661465
AFTER_BACKWARD: params_with_grad=936/942, max_grad=482.625, mean_grad=0.661465
Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 86/153 [01:33<01:12,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.264, Loss/BCELoss_step=3.650, Loss/MaskLoss_step=22.60]Epoch 0:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 86/153 [01:33<01:12,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.210, Loss/BCELoss_step=3.230, Loss/MaskLoss_step=22.90]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=383.875, mean_grad=0.775432               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=383.875, mean_grad=0.775432
AFTER_BACKWARD: params_with_grad=936/942, max_grad=383.875, mean_grad=0.775432
Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 87/153 [01:34<01:11,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.210, Loss/BCELoss_step=3.230, Loss/MaskLoss_step=22.90]Epoch 0:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 87/153 [01:34<01:11,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.250, Loss/BCELoss_step=3.510, Loss/MaskLoss_step=26.30][11/04/25 14:06:44] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=442.375, mean_grad=0.514138               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=442.375, mean_grad=0.514138
AFTER_BACKWARD: params_with_grad=936/942, max_grad=442.375, mean_grad=0.514138
Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/153 [01:35<01:10,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.250, Loss/BCELoss_step=3.510, Loss/MaskLoss_step=26.30]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 88/153 [01:35<01:10,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.227, Loss/BCELoss_step=3.330, Loss/MaskLoss_step=24.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=303.625, mean_grad=0.532363
[11/04/25 14:06:45] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=303.625, mean_grad=0.532363               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=303.625, mean_grad=0.532363
Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 89/153 [01:36<01:09,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.227, Loss/BCELoss_step=3.330, Loss/MaskLoss_step=24.00]Epoch 0:  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 89/153 [01:36<01:09,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.269, Loss/BCELoss_step=3.400, Loss/MaskLoss_step=22.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=217.25, mean_grad=0.579048
[11/04/25 14:06:46] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=217.25, mean_grad=0.579048                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=217.25, mean_grad=0.579048
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/153 [01:37<01:08,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.269, Loss/BCELoss_step=3.400, Loss/MaskLoss_step=22.50]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 90/153 [01:37<01:08,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.246, Loss/BCELoss_step=3.410, Loss/MaskLoss_step=23.20]AFTER_BACKWARD: params_with_grad=936/942, max_grad=208.562, mean_grad=0.47946
[11/04/25 14:06:47] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=208.562, mean_grad=0.47946                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=208.562, mean_grad=0.47946
Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 91/153 [01:38<01:06,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.246, Loss/BCELoss_step=3.410, Loss/MaskLoss_step=23.20]Epoch 0:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 91/153 [01:38<01:06,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.296, Loss/BCELoss_step=3.580, Loss/MaskLoss_step=19.60]AFTER_BACKWARD: params_with_grad=936/942, max_grad=489, mean_grad=0.576725
[11/04/25 14:06:48] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=489, mean_grad=0.576725                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=489, mean_grad=0.576725
Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/153 [01:38<01:05,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.296, Loss/BCELoss_step=3.580, Loss/MaskLoss_step=19.60]Epoch 0:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 92/153 [01:38<01:05,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.294, Loss/BCELoss_step=3.640, Loss/MaskLoss_step=19.30][11/04/25 14:06:49] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=378.375, mean_grad=0.510631               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=378.375, mean_grad=0.510631
AFTER_BACKWARD: params_with_grad=936/942, max_grad=378.375, mean_grad=0.510631
Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 93/153 [01:39<01:04,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.294, Loss/BCELoss_step=3.640, Loss/MaskLoss_step=19.30]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 93/153 [01:39<01:04,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.280, Loss/MaskLoss_step=24.00][11/04/25 14:06:50] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=212.625, mean_grad=0.451378               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=212.625, mean_grad=0.451378
AFTER_BACKWARD: params_with_grad=936/942, max_grad=212.625, mean_grad=0.451378
Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/153 [01:40<01:03,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.280, Loss/MaskLoss_step=24.00]Epoch 0:  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 94/153 [01:40<01:03,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.267, Loss/BCELoss_step=3.380, Loss/MaskLoss_step=21.50]AFTER_BACKWARD: params_with_grad=936/942, max_grad=776.75, mean_grad=0.518291
[11/04/25 14:06:51] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=776.75, mean_grad=0.518291                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=776.75, mean_grad=0.518291
Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/153 [01:41<01:02,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.267, Loss/BCELoss_step=3.380, Loss/MaskLoss_step=21.50]Epoch 0:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 95/153 [01:41<01:02,  0.93it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.294, Loss/BCELoss_step=3.390, Loss/MaskLoss_step=23.60][11/04/25 14:06:52] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1277.5, mean_grad=0.714625                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1277.5, mean_grad=0.714625
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1277.5, mean_grad=0.714625
Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/153 [01:42<01:00,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.294, Loss/BCELoss_step=3.390, Loss/MaskLoss_step=23.60]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 96/153 [01:42<01:00,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.189, Loss/BCELoss_step=3.310, Loss/MaskLoss_step=22.70]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=362.328, mean_grad=0.510583               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=362.328, mean_grad=0.510583
AFTER_BACKWARD: params_with_grad=936/942, max_grad=362.328, mean_grad=0.510583
Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/153 [01:43<00:59,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.189, Loss/BCELoss_step=3.310, Loss/MaskLoss_step=22.70]Epoch 0:  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 97/153 [01:43<00:59,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.258, Loss/BCELoss_step=3.350, Loss/MaskLoss_step=24.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=593.5, mean_grad=0.506889
[11/04/25 14:06:53] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=593.5, mean_grad=0.506889                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=593.5, mean_grad=0.506889
Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/153 [01:44<00:58,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.258, Loss/BCELoss_step=3.350, Loss/MaskLoss_step=24.40]Epoch 0:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 98/153 [01:44<00:58,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.207, Loss/BCELoss_step=3.140, Loss/MaskLoss_step=22.00][11/04/25 14:06:54] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=435.5, mean_grad=0.373914                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=435.5, mean_grad=0.373914
AFTER_BACKWARD: params_with_grad=936/942, max_grad=435.5, mean_grad=0.373914
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 99/153 [01:45<00:57,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.207, Loss/BCELoss_step=3.140, Loss/MaskLoss_step=22.00]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 99/153 [01:45<00:57,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.340, Loss/MaskLoss_step=21.70]AFTER_BACKWARD: params_with_grad=936/942, max_grad=473.75, mean_grad=0.518281
[11/04/25 14:06:55] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=473.75, mean_grad=0.518281                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=473.75, mean_grad=0.518281
Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/153 [01:46<00:56,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.238, Loss/BCELoss_step=3.340, Loss/MaskLoss_step=21.70]Epoch 0:  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 100/153 [01:46<00:56,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.443, Loss/BCELoss_step=3.490, Loss/MaskLoss_step=23.10][11/04/25 14:06:56] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=279.375, mean_grad=0.466141               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=279.375, mean_grad=0.466141
AFTER_BACKWARD: params_with_grad=936/942, max_grad=279.375, mean_grad=0.466141
Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 101/153 [01:47<00:55,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.443, Loss/BCELoss_step=3.490, Loss/MaskLoss_step=23.10]Epoch 0:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 101/153 [01:47<00:55,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.242, Loss/BCELoss_step=3.300, Loss/MaskLoss_step=19.50][11/04/25 14:06:57] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=776.75, mean_grad=0.662969                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=776.75, mean_grad=0.662969
AFTER_BACKWARD: params_with_grad=936/942, max_grad=776.75, mean_grad=0.662969
Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 102/153 [01:47<00:53,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.242, Loss/BCELoss_step=3.300, Loss/MaskLoss_step=19.50]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 102/153 [01:47<00:53,  0.94it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.160, Loss/MaskLoss_step=24.40]AFTER_BACKWARD: params_with_grad=936/942, max_grad=282.25, mean_grad=0.459934
[11/04/25 14:06:58] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=282.25, mean_grad=0.459934                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=282.25, mean_grad=0.459934
Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 103/153 [01:48<00:52,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.160, Loss/MaskLoss_step=24.40]Epoch 0:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 103/153 [01:48<00:52,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.309, Loss/BCELoss_step=3.180, Loss/MaskLoss_step=21.90][11/04/25 14:06:59] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=384.75, mean_grad=0.49685                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=384.75, mean_grad=0.49685
AFTER_BACKWARD: params_with_grad=936/942, max_grad=384.75, mean_grad=0.49685
Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 104/153 [01:49<00:51,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.309, Loss/BCELoss_step=3.180, Loss/MaskLoss_step=21.90]Epoch 0:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 104/153 [01:49<00:51,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.182, Loss/BCELoss_step=3.250, Loss/MaskLoss_step=22.70]AFTER_BACKWARD: params_with_grad=936/942, max_grad=279.875, mean_grad=0.517415
[11/04/25 14:07:00] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=279.875, mean_grad=0.517415               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=279.875, mean_grad=0.517415
Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 105/153 [01:50<00:50,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.182, Loss/BCELoss_step=3.250, Loss/MaskLoss_step=22.70]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 105/153 [01:50<00:50,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=21.90][11/04/25 14:07:01] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=258.531, mean_grad=0.480347               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=258.531, mean_grad=0.480347
AFTER_BACKWARD: params_with_grad=936/942, max_grad=258.531, mean_grad=0.480347
Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 106/153 [01:51<00:49,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.229, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=21.90]Epoch 0:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 106/153 [01:51<00:49,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.190, Loss/BCELoss_step=3.280, Loss/MaskLoss_step=19.00][11/04/25 14:07:02] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=510.25, mean_grad=0.570933                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=510.25, mean_grad=0.570933
AFTER_BACKWARD: params_with_grad=936/942, max_grad=510.25, mean_grad=0.570933
Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 107/153 [01:52<00:48,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.190, Loss/BCELoss_step=3.280, Loss/MaskLoss_step=19.00]Epoch 0:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 107/153 [01:52<00:48,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.180, Loss/BCELoss_step=3.190, Loss/MaskLoss_step=19.00][11/04/25 14:07:03] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=300.625, mean_grad=0.482489               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=300.625, mean_grad=0.482489
AFTER_BACKWARD: params_with_grad=936/942, max_grad=300.625, mean_grad=0.482489
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 108/153 [01:53<00:47,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.180, Loss/BCELoss_step=3.190, Loss/MaskLoss_step=19.00]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 108/153 [01:53<00:47,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.223, Loss/BCELoss_step=3.300, Loss/MaskLoss_step=19.30]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=485.125, mean_grad=0.560621               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=485.125, mean_grad=0.560621
AFTER_BACKWARD: params_with_grad=936/942, max_grad=485.125, mean_grad=0.560621
Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 109/153 [01:54<00:46,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.223, Loss/BCELoss_step=3.300, Loss/MaskLoss_step=19.30]Epoch 0:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 109/153 [01:54<00:46,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.216, Loss/BCELoss_step=3.470, Loss/MaskLoss_step=21.40][11/04/25 14:07:04] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=586.875, mean_grad=0.552182               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=586.875, mean_grad=0.552182
AFTER_BACKWARD: params_with_grad=936/942, max_grad=586.875, mean_grad=0.552182
Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/153 [01:55<00:45,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.216, Loss/BCELoss_step=3.470, Loss/MaskLoss_step=21.40]Epoch 0:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 110/153 [01:55<00:45,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.252, Loss/BCELoss_step=3.210, Loss/MaskLoss_step=21.70]AFTER_BACKWARD: params_with_grad=936/942, max_grad=528.125, mean_grad=0.747218
[11/04/25 14:07:05] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=528.125, mean_grad=0.747218               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=528.125, mean_grad=0.747218
Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/153 [01:56<00:43,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.252, Loss/BCELoss_step=3.210, Loss/MaskLoss_step=21.70]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 111/153 [01:56<00:43,  0.95it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.930, Loss/MaskLoss_step=21.40][11/04/25 14:07:06] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=631.625, mean_grad=0.680118               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=631.625, mean_grad=0.680118
AFTER_BACKWARD: params_with_grad=936/942, max_grad=631.625, mean_grad=0.680118
Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/153 [01:57<00:42,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.290, Loss/BCELoss_step=3.930, Loss/MaskLoss_step=21.40]Epoch 0:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 112/153 [01:57<00:42,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.190, Loss/BCELoss_step=3.120, Loss/MaskLoss_step=23.20][11/04/25 14:07:07] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1170.5, mean_grad=0.635213                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1170.5, mean_grad=0.635213
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1170.5, mean_grad=0.635213
Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/153 [01:58<00:41,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.190, Loss/BCELoss_step=3.120, Loss/MaskLoss_step=23.20]Epoch 0:  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 113/153 [01:58<00:41,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.173, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=17.50][11/04/25 14:07:08] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=677.25, mean_grad=0.629109                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=677.25, mean_grad=0.629109
AFTER_BACKWARD: params_with_grad=936/942, max_grad=677.25, mean_grad=0.629109
Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 114/153 [01:59<00:40,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.173, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=17.50]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 114/153 [01:59<00:40,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.193, Loss/BCELoss_step=3.080, Loss/MaskLoss_step=26.50][11/04/25 14:07:09] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=126.844, mean_grad=0.508733               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=126.844, mean_grad=0.508733
AFTER_BACKWARD: params_with_grad=936/942, max_grad=126.844, mean_grad=0.508733
Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/153 [01:59<00:39,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.193, Loss/BCELoss_step=3.080, Loss/MaskLoss_step=26.50]Epoch 0:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 115/153 [01:59<00:39,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.172, Loss/BCELoss_step=3.220, Loss/MaskLoss_step=21.00]AFTER_BACKWARD: params_with_grad=936/942, max_grad=1068.5, mean_grad=0.75293
[11/04/25 14:07:10] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1068.5, mean_grad=0.75293                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1068.5, mean_grad=0.75293
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 116/153 [02:00<00:38,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.172, Loss/BCELoss_step=3.220, Loss/MaskLoss_step=21.00]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 116/153 [02:00<00:38,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.202, Loss/BCELoss_step=3.130, Loss/MaskLoss_step=20.10]AFTER_BACKWARD: params_with_grad=936/942, max_grad=574, mean_grad=0.598194
[11/04/25 14:07:11] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=574, mean_grad=0.598194                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=574, mean_grad=0.598194
Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/153 [02:01<00:37,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.202, Loss/BCELoss_step=3.130, Loss/MaskLoss_step=20.10]Epoch 0:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 117/153 [02:01<00:37,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.209, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=22.10][11/04/25 14:07:12] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=327, mean_grad=0.479218                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=327, mean_grad=0.479218
AFTER_BACKWARD: params_with_grad=936/942, max_grad=327, mean_grad=0.479218
Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 118/153 [02:02<00:36,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.209, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=22.10]Epoch 0:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 118/153 [02:02<00:36,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.228, Loss/BCELoss_step=3.450, Loss/MaskLoss_step=17.80][11/04/25 14:07:13] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=404.75, mean_grad=0.51166                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=404.75, mean_grad=0.51166
AFTER_BACKWARD: params_with_grad=936/942, max_grad=404.75, mean_grad=0.51166
Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 119/153 [02:03<00:35,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.228, Loss/BCELoss_step=3.450, Loss/MaskLoss_step=17.80]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 119/153 [02:03<00:35,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.249, Loss/BCELoss_step=3.250, Loss/MaskLoss_step=21.90][11/04/25 14:07:14] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=618.5, mean_grad=0.655791                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=618.5, mean_grad=0.655791
AFTER_BACKWARD: params_with_grad=936/942, max_grad=618.5, mean_grad=0.655791
Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 120/153 [02:04<00:34,  0.96it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.249, Loss/BCELoss_step=3.250, Loss/MaskLoss_step=21.90]Epoch 0:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 120/153 [02:04<00:34,  0.96it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.207, Loss/BCELoss_step=3.100, Loss/MaskLoss_step=20.50][11/04/25 14:07:15] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=368.625, mean_grad=0.613651               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=368.625, mean_grad=0.613651
AFTER_BACKWARD: params_with_grad=936/942, max_grad=368.625, mean_grad=0.613651
Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 121/153 [02:05<00:33,  0.96it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.207, Loss/BCELoss_step=3.100, Loss/MaskLoss_step=20.50]Epoch 0:  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 121/153 [02:05<00:33,  0.96it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.201, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=21.60]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=175.344, mean_grad=0.480977               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=175.344, mean_grad=0.480977
AFTER_BACKWARD: params_with_grad=936/942, max_grad=175.344, mean_grad=0.480977
Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 122/153 [02:06<00:32,  0.97it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.201, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=21.60]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 122/153 [02:06<00:32,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.199, Loss/BCELoss_step=3.030, Loss/MaskLoss_step=24.90][11/04/25 14:07:16] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=344.75, mean_grad=0.67643                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=344.75, mean_grad=0.67643
AFTER_BACKWARD: params_with_grad=936/942, max_grad=344.75, mean_grad=0.67643
Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 123/153 [02:07<00:31,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.199, Loss/BCELoss_step=3.030, Loss/MaskLoss_step=24.90]Epoch 0:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 123/153 [02:07<00:31,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.187, Loss/BCELoss_step=3.120, Loss/MaskLoss_step=20.80][11/04/25 14:07:17] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=602.25, mean_grad=0.670956                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=602.25, mean_grad=0.670956
AFTER_BACKWARD: params_with_grad=936/942, max_grad=602.25, mean_grad=0.670956
Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 124/153 [02:08<00:29,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.187, Loss/BCELoss_step=3.120, Loss/MaskLoss_step=20.80]Epoch 0:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 124/153 [02:08<00:29,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.186, Loss/BCELoss_step=3.170, Loss/MaskLoss_step=21.10][11/04/25 14:07:18] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=487.25, mean_grad=0.726429                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=487.25, mean_grad=0.726429
AFTER_BACKWARD: params_with_grad=936/942, max_grad=487.25, mean_grad=0.726429
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/153 [02:09<00:28,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.186, Loss/BCELoss_step=3.170, Loss/MaskLoss_step=21.10]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 125/153 [02:09<00:28,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.198, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=20.10][11/04/25 14:07:19] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=188.859, mean_grad=0.382576               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=188.859, mean_grad=0.382576
AFTER_BACKWARD: params_with_grad=936/942, max_grad=188.859, mean_grad=0.382576
Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/153 [02:09<00:27,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.198, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=20.10]Epoch 0:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 126/153 [02:09<00:27,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.155, Loss/BCELoss_step=3.030, Loss/MaskLoss_step=20.10][11/04/25 14:07:20] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=260.875, mean_grad=0.409556               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=260.875, mean_grad=0.409556
AFTER_BACKWARD: params_with_grad=936/942, max_grad=260.875, mean_grad=0.409556
Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/153 [02:10<00:26,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.155, Loss/BCELoss_step=3.030, Loss/MaskLoss_step=20.10]Epoch 0:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 127/153 [02:10<00:26,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.159, Loss/BCELoss_step=2.950, Loss/MaskLoss_step=20.70][11/04/25 14:07:21] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=601.75, mean_grad=0.601529                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=601.75, mean_grad=0.601529
AFTER_BACKWARD: params_with_grad=936/942, max_grad=601.75, mean_grad=0.601529
Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/153 [02:11<00:25,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.159, Loss/BCELoss_step=2.950, Loss/MaskLoss_step=20.70]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 128/153 [02:11<00:25,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.169, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=20.80][11/04/25 14:07:22] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=275.125, mean_grad=0.473585               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=275.125, mean_grad=0.473585
AFTER_BACKWARD: params_with_grad=936/942, max_grad=275.125, mean_grad=0.473585
Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 129/153 [02:12<00:24,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.169, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=20.80]Epoch 0:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 129/153 [02:12<00:24,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.167, Loss/BCELoss_step=3.010, Loss/MaskLoss_step=19.00][11/04/25 14:07:23] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=310.25, mean_grad=0.516367                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=310.25, mean_grad=0.516367
AFTER_BACKWARD: params_with_grad=936/942, max_grad=310.25, mean_grad=0.516367
Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 130/153 [02:13<00:23,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.167, Loss/BCELoss_step=3.010, Loss/MaskLoss_step=19.00]Epoch 0:  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 130/153 [02:13<00:23,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.173, Loss/BCELoss_step=3.040, Loss/MaskLoss_step=21.80][11/04/25 14:07:24] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=326.75, mean_grad=0.653284                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=326.75, mean_grad=0.653284
AFTER_BACKWARD: params_with_grad=936/942, max_grad=326.75, mean_grad=0.653284
Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 131/153 [02:14<00:22,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.173, Loss/BCELoss_step=3.040, Loss/MaskLoss_step=21.80]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 131/153 [02:14<00:22,  0.97it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.160, Loss/BCELoss_step=2.960, Loss/MaskLoss_step=18.60]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=378.875, mean_grad=0.395138               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=378.875, mean_grad=0.395138
AFTER_BACKWARD: params_with_grad=936/942, max_grad=378.875, mean_grad=0.395138
Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/153 [02:15<00:21,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.160, Loss/BCELoss_step=2.960, Loss/MaskLoss_step=18.60]Epoch 0:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 132/153 [02:15<00:21,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.169, Loss/BCELoss_step=3.060, Loss/MaskLoss_step=15.70][11/04/25 14:07:25] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=139.469, mean_grad=0.39801                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=139.469, mean_grad=0.39801
AFTER_BACKWARD: params_with_grad=936/942, max_grad=139.469, mean_grad=0.39801
Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 133/153 [02:16<00:20,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.169, Loss/BCELoss_step=3.060, Loss/MaskLoss_step=15.70]Epoch 0:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 133/153 [02:16<00:20,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.166, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=19.00][11/04/25 14:07:26] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=270.062, mean_grad=0.444747               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=270.062, mean_grad=0.444747
AFTER_BACKWARD: params_with_grad=936/942, max_grad=270.062, mean_grad=0.444747
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/153 [02:17<00:19,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.166, Loss/BCELoss_step=3.200, Loss/MaskLoss_step=19.00]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 134/153 [02:17<00:19,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.166, Loss/BCELoss_step=2.910, Loss/MaskLoss_step=18.30][11/04/25 14:07:27] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=355.125, mean_grad=0.391358               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=355.125, mean_grad=0.391358
AFTER_BACKWARD: params_with_grad=936/942, max_grad=355.125, mean_grad=0.391358
Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 135/153 [02:18<00:18,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.166, Loss/BCELoss_step=2.910, Loss/MaskLoss_step=18.30]Epoch 0:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 135/153 [02:18<00:18,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.160, Loss/BCELoss_step=2.970, Loss/MaskLoss_step=17.90][11/04/25 14:07:28] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=540.5, mean_grad=0.601814                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=540.5, mean_grad=0.601814
AFTER_BACKWARD: params_with_grad=936/942, max_grad=540.5, mean_grad=0.601814
Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 136/153 [02:18<00:17,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.160, Loss/BCELoss_step=2.970, Loss/MaskLoss_step=17.90]Epoch 0:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 136/153 [02:18<00:17,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.177, Loss/BCELoss_step=3.330, Loss/MaskLoss_step=20.50][11/04/25 14:07:29] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=278.125, mean_grad=0.410252               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.125, mean_grad=0.410252
AFTER_BACKWARD: params_with_grad=936/942, max_grad=278.125, mean_grad=0.410252
Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 137/153 [02:19<00:16,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.177, Loss/BCELoss_step=3.330, Loss/MaskLoss_step=20.50]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 137/153 [02:19<00:16,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.161, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=18.30][11/04/25 14:07:30] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=346.188, mean_grad=0.332296               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=346.188, mean_grad=0.332296
AFTER_BACKWARD: params_with_grad=936/942, max_grad=346.188, mean_grad=0.332296
Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 138/153 [02:20<00:15,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.161, Loss/BCELoss_step=3.070, Loss/MaskLoss_step=18.30]Epoch 0:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 138/153 [02:20<00:15,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.148, Loss/BCELoss_step=3.040, Loss/MaskLoss_step=16.00][11/04/25 14:07:31] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=418.375, mean_grad=0.505876               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=418.375, mean_grad=0.505876
AFTER_BACKWARD: params_with_grad=936/942, max_grad=418.375, mean_grad=0.505876
Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 139/153 [02:21<00:14,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.148, Loss/BCELoss_step=3.040, Loss/MaskLoss_step=16.00]Epoch 0:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 139/153 [02:21<00:14,  0.98it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.149, Loss/BCELoss_step=2.890, Loss/MaskLoss_step=18.20][11/04/25 14:07:32] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=914, mean_grad=0.707761                   
AFTER_BACKWARD: params_with_grad=936/942, max_grad=914, mean_grad=0.707761
AFTER_BACKWARD: params_with_grad=936/942, max_grad=914, mean_grad=0.707761
Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/153 [02:22<00:13,  0.98it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.149, Loss/BCELoss_step=2.890, Loss/MaskLoss_step=18.20]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 140/153 [02:22<00:13,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.136, Loss/BCELoss_step=3.000, Loss/MaskLoss_step=23.90][11/04/25 14:07:33] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=485.375, mean_grad=0.532415               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=485.375, mean_grad=0.532415
AFTER_BACKWARD: params_with_grad=936/942, max_grad=485.375, mean_grad=0.532415
Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/153 [02:23<00:12,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.136, Loss/BCELoss_step=3.000, Loss/MaskLoss_step=23.90]Epoch 0:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 141/153 [02:23<00:12,  0.98it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.870, Loss/MaskLoss_step=16.60]                    INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=274.562, mean_grad=0.326853               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=274.562, mean_grad=0.326853
AFTER_BACKWARD: params_with_grad=936/942, max_grad=274.562, mean_grad=0.326853
Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/153 [02:24<00:11,  0.98it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.870, Loss/MaskLoss_step=16.60]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 142/153 [02:24<00:11,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.131, Loss/BCELoss_step=2.760, Loss/MaskLoss_step=18.40][11/04/25 14:07:34] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=611.75, mean_grad=0.479182                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=611.75, mean_grad=0.479182
AFTER_BACKWARD: params_with_grad=936/942, max_grad=611.75, mean_grad=0.479182
Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/153 [02:25<00:10,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.131, Loss/BCELoss_step=2.760, Loss/MaskLoss_step=18.40]Epoch 0:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 143/153 [02:25<00:10,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.920, Loss/MaskLoss_step=18.70][11/04/25 14:07:35] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=362.125, mean_grad=0.430906               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=362.125, mean_grad=0.430906
AFTER_BACKWARD: params_with_grad=936/942, max_grad=362.125, mean_grad=0.430906
Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 144/153 [02:26<00:09,  0.98it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.920, Loss/MaskLoss_step=18.70]Epoch 0:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 144/153 [02:26<00:09,  0.98it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.136, Loss/BCELoss_step=2.810, Loss/MaskLoss_step=19.70][11/04/25 14:07:36] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=303.5, mean_grad=0.359955                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=303.5, mean_grad=0.359955
AFTER_BACKWARD: params_with_grad=936/942, max_grad=303.5, mean_grad=0.359955
Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 145/153 [02:27<00:08,  0.99it/s, Loss/BoxLoss_step=9.370, Loss/DFLLoss_step=0.136, Loss/BCELoss_step=2.810, Loss/MaskLoss_step=19.70]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 145/153 [02:27<00:08,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.143, Loss/BCELoss_step=2.930, Loss/MaskLoss_step=18.10][11/04/25 14:07:37] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=384.75, mean_grad=0.387295                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=384.75, mean_grad=0.387295
AFTER_BACKWARD: params_with_grad=936/942, max_grad=384.75, mean_grad=0.387295
Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 146/153 [02:28<00:07,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.143, Loss/BCELoss_step=2.930, Loss/MaskLoss_step=18.10]Epoch 0:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 146/153 [02:28<00:07,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.135, Loss/BCELoss_step=2.760, Loss/MaskLoss_step=20.90][11/04/25 14:07:38] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=280.406, mean_grad=0.350765               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=280.406, mean_grad=0.350765
AFTER_BACKWARD: params_with_grad=936/942, max_grad=280.406, mean_grad=0.350765
Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 147/153 [02:28<00:06,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.135, Loss/BCELoss_step=2.760, Loss/MaskLoss_step=20.90]Epoch 0:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 147/153 [02:28<00:06,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.129, Loss/BCELoss_step=2.920, Loss/MaskLoss_step=18.90][11/04/25 14:07:39] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=232.469, mean_grad=0.440254               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=232.469, mean_grad=0.440254
AFTER_BACKWARD: params_with_grad=936/942, max_grad=232.469, mean_grad=0.440254
Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 148/153 [02:29<00:05,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.129, Loss/BCELoss_step=2.920, Loss/MaskLoss_step=18.90]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 148/153 [02:29<00:05,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.183, Loss/BCELoss_step=2.850, Loss/MaskLoss_step=22.20][11/04/25 14:07:40] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=591.25, mean_grad=0.492417                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=591.25, mean_grad=0.492417
AFTER_BACKWARD: params_with_grad=936/942, max_grad=591.25, mean_grad=0.492417
Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/153 [02:30<00:04,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.183, Loss/BCELoss_step=2.850, Loss/MaskLoss_step=22.20]Epoch 0:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/153 [02:30<00:04,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.183, Loss/BCELoss_step=2.830, Loss/MaskLoss_step=18.00][11/04/25 14:07:41] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=1019.25, mean_grad=0.642261               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1019.25, mean_grad=0.642261
AFTER_BACKWARD: params_with_grad=936/942, max_grad=1019.25, mean_grad=0.642261
Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 150/153 [02:31<00:03,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.183, Loss/BCELoss_step=2.830, Loss/MaskLoss_step=18.00]Epoch 0:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 150/153 [02:31<00:03,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.850, Loss/MaskLoss_step=15.80][11/04/25 14:07:42] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=439.625, mean_grad=0.458542               
AFTER_BACKWARD: params_with_grad=936/942, max_grad=439.625, mean_grad=0.458542
AFTER_BACKWARD: params_with_grad=936/942, max_grad=439.625, mean_grad=0.458542
Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 151/153 [02:32<00:02,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.144, Loss/BCELoss_step=2.850, Loss/MaskLoss_step=15.80]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 151/153 [02:32<00:02,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.131, Loss/BCELoss_step=2.910, Loss/MaskLoss_step=14.40][11/04/25 14:07:43] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=601.5, mean_grad=0.518401                 
AFTER_BACKWARD: params_with_grad=936/942, max_grad=601.5, mean_grad=0.518401
AFTER_BACKWARD: params_with_grad=936/942, max_grad=601.5, mean_grad=0.518401
Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 152/153 [02:33<00:01,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.131, Loss/BCELoss_step=2.910, Loss/MaskLoss_step=14.40]Epoch 0:  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 152/153 [02:33<00:01,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.140, Loss/BCELoss_step=2.740, Loss/MaskLoss_step=15.20][11/04/25 14:07:44] INFO     AFTER_BACKWARD:                       solver.py:293
                             params_with_grad=936/942,                          
                             max_grad=309.75, mean_grad=0.406433                
AFTER_BACKWARD: params_with_grad=936/942, max_grad=309.75, mean_grad=0.406433
AFTER_BACKWARD: params_with_grad=936/942, max_grad=309.75, mean_grad=0.406433
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [02:34<00:00,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.140, Loss/BCELoss_step=2.740, Loss/MaskLoss_step=15.20]Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [02:34<00:00,  0.99it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.132, Loss/BCELoss_step=2.730, Loss/MaskLoss_step=16.50]
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation: |          | 0/? [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/66 [00:00<?, ?it/s][AError in validation step: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
Error type: <class 'RuntimeError'>
Traceback (most recent call last):
  File "/kaggle/working/YOLO/yolo/tools/solver.py", line 97, in validation_step
    logger.info(f"- Scores: min={pred['scores'].min().item():.4f}, max={pred['scores'].max().item():.4f}" if 'scores' in pred else 'No scores')
                                 ^^^^^^^^^^^^^^^^^^^^
RuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.

[11/04/25 14:07:52] INFO     === DEBUG: Model Output Structure ===  solver.py:78
                    INFO     Main: [<class 'list'>, <class 'list'>] solver.py:81
                    INFO     AUX: [<class 'list'>, <class 'list'>]  solver.py:81
                    INFO                                            solver.py:92
                             === DEBUG: Post-Process Output ===                 
                    INFO                                            solver.py:94
                             Imagen 0:                                          
                    INFO     - Boxes shape: torch.Size([0, 4])      solver.py:95
                    INFO     - Labels shape: torch.Size([0])        solver.py:96
                    ERROR    Error in validation step: min():      solver.py:187
                             Expected reduction dim to be                       
                             specified for input.numel() == 0.                  
                             Specify the reduction dim with the                 
                             'dim' argument.                                    
                    ERROR    Error type: <class 'RuntimeError'>    solver.py:188
                    ERROR    Traceback (most recent call last):    solver.py:191
                               File                                             
                             "/kaggle/working/YOLO/yolo/tools/solv              
                             er.py", line 97, in validation_step                
                                 logger.info(f"- Scores:                        
                             min={pred['scores'].min().item():.4f}              
                             ,                                                  
                             max={pred['scores'].max().item():.4f}              
                             " if 'scores' in pred else 'No                     
                             scores')                                           
                                                              ^^^^              
                             ^^^^^^^^^^^^^^^^                                   
                             RuntimeError: min(): Expected                      
                             reduction dim to be specified for                  
                             input.numel() == 0. Specify the                    
                             reduction dim with the 'dim'                       
                             argument.                                          
                                                                                
Error executing job with overrides: ['task=train', 'dataset=dental_dataset', 'model=v9-c-seg', 'weight=False', 'use_wandb=False']
Traceback (most recent call last):
  File "/kaggle/working/YOLO/yolo/lazy.py", line 39, in main
    trainer.fit(model)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 411, in validation_step
    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/YOLO/yolo/tools/solver.py", line 97, in validation_step
    logger.info(f"- Scores: min={pred['scores'].min().item():.4f}, max={pred['scores'].max().item():.4f}" if 'scores' in pred else 'No scores')
                                 ^^^^^^^^^^^^^^^^^^^^
RuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Error executing job with overrides: ['task=train', 'dataset=dental_dataset', 'model=v9-c-seg', 'weight=False', 'use_wandb=False']
Traceback (most recent call last):
  File "/kaggle/working/YOLO/yolo/lazy.py", line 39, in main
    trainer.fit(model)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 560, in fit
    call._call_and_handle_interrupt(
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py", line 48, in _call_and_handle_interrupt
    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/launchers/subprocess_script.py", line 105, in launch
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 598, in _fit_impl
    self._run(model, ckpt_path=ckpt_path)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 1011, in _run
    results = self._run_stage()
              ^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py", line 1055, in _run_stage
    self.fit_loop.run()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py", line 216, in run
    self.advance()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py", line 458, in advance
    self.epoch_loop.run(self._data_fetcher)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 153, in run
    self.on_advance_end(data_fetcher)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py", line 398, in on_advance_end
    self.val_loop.run()
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py", line 179, in _decorator
    return loop_run(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 145, in run
    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py", line 437, in _evaluation_step
    output = call._call_strategy_hook(trainer, hook_name, *step_args)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py", line 329, in _call_strategy_hook
    output = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 411, in validation_step
    return self._forward_redirection(self.model, self.lightning_module, "validation_step", *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 641, in __call__
    wrapper_output = wrapper_module(*args, **kwargs)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py", line 1648, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py", line 1474, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py", line 1784, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py", line 634, in wrapped_forward
    out = method(*_args, **_kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/kaggle/working/YOLO/yolo/tools/solver.py", line 97, in validation_step
    logger.info(f"- Scores: min={pred['scores'].min().item():.4f}, max={pred['scores'].max().item():.4f}" if 'scores' in pred else 'No scores')
                                 ^^^^^^^^^^^^^^^^^^^^
RuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [02:45<00:00,  0.92it/s, Loss/BoxLoss_step=9.380, Loss/DFLLoss_step=0.132, Loss/BCELoss_step=2.730, Loss/MaskLoss_step=16.50]

                                                               [A